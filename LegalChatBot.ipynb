{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10863022,"sourceType":"datasetVersion","datasetId":6748424},{"sourceId":10873475,"sourceType":"datasetVersion","datasetId":6755873},{"sourceId":10877017,"sourceType":"datasetVersion","datasetId":6758197},{"sourceId":11396784,"sourceType":"datasetVersion","datasetId":7137654}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git config --global user.name \"rachitneedcse\"\n!git config --global user.email \"rachitguptacse.098@gmail.com\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T11:05:44.287134Z","iopub.execute_input":"2025-08-14T11:05:44.287501Z","iopub.status.idle":"2025-08-14T11:05:44.586633Z","shell.execute_reply.started":"2025-08-14T11:05:44.287435Z","shell.execute_reply":"2025-08-14T11:05:44.585595Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# dependecy","metadata":{}},{"cell_type":"code","source":"pip install sentence-transformers langchain_community unstructured faiss-cpu Together together\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:13:14.248250Z","iopub.execute_input":"2025-08-12T08:13:14.248733Z","iopub.status.idle":"2025-08-12T08:13:45.105904Z","shell.execute_reply.started":"2025-08-12T08:13:14.248671Z","shell.execute_reply":"2025-08-12T08:13:45.105020Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\nCollecting langchain_community\n  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\nCollecting unstructured\n  Downloading unstructured-0.18.11-py3-none-any.whl.metadata (24 kB)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.11.0.post1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\nCollecting Together\n  Downloading together-1.5.23-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.29.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\nCollecting langchain-core<1.0.0,>=0.3.66 (from langchain_community)\n  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\nCollecting langchain<1.0.0,>=0.3.26 (from langchain_community)\n  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.12)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.3)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\nRequirement already satisfied: charset-normalizer in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.4.1)\nCollecting filetype (from unstructured)\n  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\nCollecting python-magic (from unstructured)\n  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.3.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.2.4)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\nRequirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.14.1)\nCollecting python-iso639 (from unstructured)\n  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\nCollecting langdetect (from unstructured)\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting rapidfuzz (from unstructured)\n  Downloading rapidfuzz-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting backoff (from unstructured)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.2)\nCollecting unstructured-client (from unstructured)\n  Downloading unstructured_client-0.42.2-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.17.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.9.5)\nCollecting python-oxmsg (from unstructured)\n  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\nRequirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from Together) (8.1.7)\nRequirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from Together) (0.2.0)\nRequirement already satisfied: filelock<4.0.0,>=3.13.1 in /usr/local/lib/python3.10/dist-packages (from Together) (3.17.0)\nCollecting Pillow (from sentence-transformers)\n  Downloading pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from Together) (2.11.0a2)\nRequirement already satisfied: rich<15.0.0,>=13.8.1 in /usr/local/lib/python3.10/dist-packages (from Together) (13.9.4)\nRequirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from Together) (0.9.0)\nRequirement already satisfied: typer<0.16,>=0.9 in /usr/local/lib/python3.10/dist-packages (from Together) (0.15.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\nCollecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain<1.0.0,>=0.3.26->langchain_community)\n  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\nCollecting async-timeout<6.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\nCollecting langsmith>=0.1.125 (from langchain_community)\n  Downloading langsmith-0.4.13-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\nRequirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith>=0.1.125->langchain_community) (3.10.12)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\nCollecting zstandard>=0.23.0 (from langsmith>=0.1.125->langchain_community)\n  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain_community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain_community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain_community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain_community) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain_community) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain_community) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.3->Together) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.3->Together) (2.29.0)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nCollecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0.0,>=13.8.1->Together) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0.0,>=13.8.1->Together) (2.19.1)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.16,>=0.9->Together) (1.5.4)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.6)\nRequirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib->unstructured) (1.17.0)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib->unstructured) (0.5.1)\nRequirement already satisfied: olefile in /usr/local/lib/python3.10/dist-packages (from python-oxmsg->unstructured) (0.47)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nCollecting aiofiles>=24.1.0 (from unstructured-client->unstructured)\n  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (44.0.1)\nCollecting httpcore>=1.0.9 (from unstructured-client->unstructured)\n  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\nCollecting pydantic<3.0.0,>=2.6.3 (from Together)\n  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (5.3.0)\nCollecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.6.3->Together)\n  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\nCollecting h11>=0.16 (from httpcore>=1.0.9->unstructured-client->unstructured)\n  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (3.7.1)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.8.1->Together) (0.1.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.26.2->langchain_community) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain_community) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.2.2)\nDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading unstructured-0.18.11-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading faiss_cpu-1.11.0.post1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading together-1.5.23-py3-none-any.whl (102 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.7/102.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\nDownloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.5/443.5 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langsmith-0.4.13-py3-none-any.whl (372 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m372.7/372.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nDownloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\nDownloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\nDownloading rapidfuzz-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading unstructured_client-0.42.2-py3-none-any.whl (207 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\nDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\nDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\nDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\nBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=5343a2d059f1d7443c117ed642120efcc295a9a02fbca9548e5837aa9d0ba83c\n  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\nSuccessfully built langdetect\nInstalling collected packages: filetype, zstandard, typing-inspection, rapidfuzz, python-oxmsg, python-magic, python-iso639, python-dotenv, pydantic-core, Pillow, langdetect, httpx-sse, h11, backoff, async-timeout, aiofiles, pydantic, httpcore, pydantic-settings, unstructured-client, langsmith, langchain-core, langchain-text-splitters, langchain, unstructured, Together, langchain_community, faiss-cpu\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.29.0\n    Uninstalling pydantic_core-2.29.0:\n      Successfully uninstalled pydantic_core-2.29.0\n  Attempting uninstall: Pillow\n    Found existing installation: pillow 11.0.0\n    Uninstalling pillow-11.0.0:\n      Successfully uninstalled pillow-11.0.0\n  Attempting uninstall: h11\n    Found existing installation: h11 0.14.0\n    Uninstalling h11-0.14.0:\n      Successfully uninstalled h11-0.14.0\n  Attempting uninstall: async-timeout\n    Found existing installation: async-timeout 5.0.1\n    Uninstalling async-timeout-5.0.1:\n      Successfully uninstalled async-timeout-5.0.1\n  Attempting uninstall: aiofiles\n    Found existing installation: aiofiles 22.1.0\n    Uninstalling aiofiles-22.1.0:\n      Successfully uninstalled aiofiles-22.1.0\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.11.0a2\n    Uninstalling pydantic-2.11.0a2:\n      Successfully uninstalled pydantic-2.11.0a2\n  Attempting uninstall: httpcore\n    Found existing installation: httpcore 1.0.7\n    Uninstalling httpcore-1.0.7:\n      Successfully uninstalled httpcore-1.0.7\n  Attempting uninstall: langsmith\n    Found existing installation: langsmith 0.2.3\n    Uninstalling langsmith-0.2.3:\n      Successfully uninstalled langsmith-0.2.3\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.25\n    Uninstalling langchain-core-0.3.25:\n      Successfully uninstalled langchain-core-0.3.25\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.3\n    Uninstalling langchain-text-splitters-0.3.3:\n      Successfully uninstalled langchain-text-splitters-0.3.3\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.12\n    Uninstalling langchain-0.3.12:\n      Successfully uninstalled langchain-0.3.12\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\nypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 24.1.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Pillow-11.3.0 Together-1.5.23 aiofiles-24.1.0 async-timeout-4.0.3 backoff-2.2.1 faiss-cpu-1.11.0.post1 filetype-1.2.0 h11-0.16.0 httpcore-1.0.9 httpx-sse-0.4.1 langchain-0.3.27 langchain-core-0.3.74 langchain-text-splitters-0.3.9 langchain_community-0.3.27 langdetect-1.0.9 langsmith-0.4.13 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-settings-2.10.1 python-dotenv-1.1.1 python-iso639-2025.2.18 python-magic-0.4.27 python-oxmsg-0.0.2 rapidfuzz-3.13.0 typing-inspection-0.4.1 unstructured-0.18.11 unstructured-client-0.42.2 zstandard-0.23.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Code","metadata":{}},{"cell_type":"code","source":"import json\n\n\nwith open(\"/kaggle/input/final-database/legal_database.json\", \"r\", encoding=\"utf-8\") as f:\n    qa_data = json.load(f)\n\n\nquestions = [item[\"question\"] for item in qa_data]\nanswers = {item[\"question\"]: item[\"answer\"] for item in qa_data}  # Mapping Q -> A\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:13:45.107080Z","iopub.execute_input":"2025-08-12T08:13:45.107351Z","iopub.status.idle":"2025-08-12T08:13:45.268428Z","shell.execute_reply.started":"2025-08-12T08:13:45.107326Z","shell.execute_reply":"2025-08-12T08:13:45.267809Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\n\nmodel = SentenceTransformer(\"sentence-transformers/msmarco-distilbert-base-v4\")  \nmodel.save(\"qa_model\")\n\nquestion_embeddings = model.encode(questions, convert_to_tensor=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:13:45.269972Z","iopub.execute_input":"2025-08-12T08:13:45.270258Z","iopub.status.idle":"2025-08-12T08:14:37.713955Z","shell.execute_reply.started":"2025-08-12T08:13:45.270238Z","shell.execute_reply":"2025-08-12T08:14:37.713157Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"472fe20853ec479dbed15f0c0fdbdf0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c27b2d1ca13f46c6b265c1b091e4bb48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6dc2c81416a40878f856e5820d7ee72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be14f4e93f6b40899bb6f069d9f39fc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/545 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cb2b6d33aaa4ac5873cd98ec16c8657"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fc27d13534e4e6fa52e48ebb04b65c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/319 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9951192862464734a0585fd87256e4f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22afeb0fb3e44996bbedbd3726c45740"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6f4d2d2588b40a7b27312aa6423f0fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69da387707d847c6a2f4cc8a789ab68e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54c8ee5392db44899f0a39aba3f62e32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/455 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1c0f4be9e94454498e2434b58b11499"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import torch\n\n\ntorch.save({\n    \"embeddings\": question_embeddings,\n    \"questions\": questions\n}, \"question_embeddings.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:14:37.715425Z","iopub.execute_input":"2025-08-12T08:14:37.716222Z","iopub.status.idle":"2025-08-12T08:14:37.840835Z","shell.execute_reply.started":"2025-08-12T08:14:37.716170Z","shell.execute_reply":"2025-08-12T08:14:37.840127Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import numpy as np\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import DirectoryLoader, PyPDFDirectoryLoader\n\n\ntxt_loader = DirectoryLoader('/kaggle/input/indian-law', glob=\"*.txt\")\npdf_loader = PyPDFDirectoryLoader('/kaggle/input/indian-law')  \n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=100)\n\n\ntxt_documents = txt_loader.load_and_split(text_splitter)\npdf_documents = pdf_loader.load_and_split(text_splitter)\n\n\nall_documents = txt_documents + pdf_documents\nchunks = [doc.page_content for doc in all_documents]\n\n\nnp.save(\"legal_chunks.npy\", np.array(chunks, dtype=object))\n\nprint(f\"✅ Processed {len(chunks)} legal text chunks from TXT & PDF.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:14:37.841681Z","iopub.execute_input":"2025-08-12T08:14:37.842012Z","iopub.status.idle":"2025-08-12T08:16:13.103934Z","shell.execute_reply.started":"2025-08-12T08:14:37.841983Z","shell.execute_reply":"2025-08-12T08:16:13.102906Z"}},"outputs":[{"name":"stdout","text":"✅ Processed 20628 legal text chunks from TXT & PDF.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"chunks[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:16:13.104965Z","iopub.execute_input":"2025-08-12T08:16:13.105777Z","iopub.status.idle":"2025-08-12T08:16:13.111169Z","shell.execute_reply.started":"2025-08-12T08:16:13.105739Z","shell.execute_reply":"2025-08-12T08:16:13.110180Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'[Received the assent of the Governor-General on October 6, 1860.]\\n\\nCHAPTER I INTRODUCTION\\n\\nThe Indian Penal Code was drafted by the First Indian Law Commission presided over by Lord Thomas Babington Macaulay. The draft underwent further revision at the hands of well-known jurists, like Sir Barnes Peacock, and was completed in 1850. The Indian Penal Code was passed by the then Legislature on 6 October 1860 and was enacted as Act No. XLV of 1860.'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from langchain_community.embeddings import HuggingFaceEmbeddings\n\n\n\nembedding_model = HuggingFaceEmbeddings(model_name=\"law-ai/InLegalBERT\")\n\n\nlegal_embeddings = np.array([embedding_model.embed_query(chunk) for chunk in chunks])\n\n\nnp.save(\"legal_embeddings.npy\", legal_embeddings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:16:13.111938Z","iopub.execute_input":"2025-08-12T08:16:13.112137Z","iopub.status.idle":"2025-08-12T08:20:25.131351Z","shell.execute_reply.started":"2025-08-12T08:16:13.112119Z","shell.execute_reply":"2025-08-12T08:20:25.130538Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-7-d78e25d61bc3>:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embedding_model = HuggingFaceEmbeddings(model_name=\"law-ai/InLegalBERT\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/671 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1e160d2b1904664878c597072d51226"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/534M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b30acb44712450ab093c65fcf5ed345"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/516 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a442c252e6ab4236af7ce2b07e42e68a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a711ee50813849a3923e69b536dfb678"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86881bdde9044c1fb955a3df024d15ed"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"model_path = \"legal_model\"\nembedding_model.client.save_pretrained(model_path) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:20:25.132416Z","iopub.execute_input":"2025-08-12T08:20:25.132832Z","iopub.status.idle":"2025-08-12T08:20:26.184662Z","shell.execute_reply.started":"2025-08-12T08:20:25.132790Z","shell.execute_reply":"2025-08-12T08:20:26.183995Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import faiss\n\nlegal_embeddings = np.load(\"/kaggle/working/legal_embeddings.npy\", allow_pickle=True)\ndim = legal_embeddings.shape[1]\n\n\nfaiss_index = faiss.IndexFlatL2(dim)\n\n\nfaiss_index.add(legal_embeddings)\n\n\nfaiss.write_index(faiss_index, \"legal_faiss.index\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:20:26.187375Z","iopub.execute_input":"2025-08-12T08:20:26.187605Z","iopub.status.idle":"2025-08-12T08:20:26.400948Z","shell.execute_reply.started":"2025-08-12T08:20:26.187584Z","shell.execute_reply":"2025-08-12T08:20:26.399958Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import together\ntogether.api_key = \"tgp_v1_hcAYeb5IquESKVQOsx6_wbAn0jkRNJTWHnNipFUTIlI\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:20:26.403089Z","iopub.execute_input":"2025-08-12T08:20:26.403466Z","iopub.status.idle":"2025-08-12T08:20:26.548406Z","shell.execute_reply.started":"2025-08-12T08:20:26.403435Z","shell.execute_reply":"2025-08-12T08:20:26.547762Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import json\nimport torch\nimport faiss\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n\nmodel = SentenceTransformer(\"/kaggle/working/qa_model\")\nembedding_model = HuggingFaceEmbeddings(model_name=\"/kaggle/working/legal_model\")\n\n\ndata = torch.load(\"question_embeddings.pt\",weights_only=False)\nquestion_embeddings = data[\"embeddings\"]\nquestions = data[\"questions\"]\nfaiss_index = faiss.read_index(\"/kaggle/working/legal_faiss.index\")\nlegal_chunks = np.load(\"/kaggle/working/legal_chunks.npy\", allow_pickle=True)\n\n\n\ndef find_closest_match(user_question):\n    \n    user_embedding = model.encode(user_question, convert_to_tensor=True)\n    \n    similarities = util.pytorch_cos_sim(user_embedding, question_embeddings)\n    best_match_idx = similarities.argmax().item()\n    \n    best_match_question = questions[best_match_idx]\n    similarity_score = similarities[0][best_match_idx].item()\n\n    return best_match_question, similarity_score\n\n\ndef search_legal_docs(query,top_k=5):\n    \n    query_embedding = np.array(embedding_model.embed_query(query)).reshape(1, -1)\n    distances, indices = faiss_index.search(query_embedding, top_k)\n    \n    retrieved_chunks = [legal_chunks[idx] for idx in indices[0]]\n\n    \n    query_vector = torch.tensor(query_embedding, dtype=torch.float32)\n    chunk_vectors = torch.tensor([embedding_model.embed_query(chunk) for chunk in retrieved_chunks], dtype=torch.float32)\n\n    scores = util.pytorch_cos_sim(query_vector, chunk_vectors)[0]\n    ranked_chunks = sorted(zip(retrieved_chunks, scores), key=lambda x: x[1], reverse=True)\n    best_chunk, _ = ranked_chunks[0]\n    model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Choose another if needed\n\n    prompt = f\"\"\"\n    As a legal chatbot specializing in the Indian Penal Code, you are tasked with providing highly accurate and contextually appropriate responses. Ensure your answers meet these criteria:\n- First of all state the law in which the context comes in. Then Respond in a 5 bullet-point format to clearly delineate distinct aspects of the legal query.\n- Each point should accurately reflect the legal provision in question, avoiding over-specificity unless directly relevant to the user's query.\n- Clarify the general applicability of the legal rules or sections mentioned, highlighting any common misconceptions or frequently misunderstood aspects.\n- Limit responses to essential information that directly addresses the user's question, providing concise yet comprehensive explanations.\n- Avoid assuming specific contexts or details not provided in the query, focusing on delivering universally applicable legal interpretations unless otherwise specified.\n\n\nCONTEXT: {best_chunk}\n\nQUESTION: {query}\nANSWER:\n\n\n    \"\"\"\n    \n    response = together.Complete.create(\n        model=model_name,\n        prompt=prompt,\n        max_tokens=400,\n        temperature=1\n    )\n    \n\n    if \"choices\" in response and response[\"choices\"]:\n        \n        return response[\"choices\"][0][\"text\"].strip()\n    else:\n        return \"Error: No response received from Together AI.\"\n\ndef get_answer(user_question):\n    \n    best_match_question, similarity_score = find_closest_match(user_question)\n\n    if similarity_score >= 0.7:\n        return f\"✅ Exact Match Found({similarity_score:.2f}):\\n{answers[best_match_question]}\"\n\n    elif 0.3 < similarity_score < 0.7:\n        top_chunks = search_legal_docs(user_question)\n        return f\"⚖️ Legal Text Found:\\n{top_chunks}\"  \n\n    else:\n        return \"Sorry cant give response at the moment.\"  \n\n\nuser_question = \"i got in an accident. What should i do?\"\nresponse = get_answer(user_question)\nprint(response)\n","metadata":{"trusted":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2025-08-12T08:20:26.549142Z","iopub.execute_input":"2025-08-12T08:20:26.549364Z","iopub.status.idle":"2025-08-12T08:20:28.901001Z","shell.execute_reply.started":"2025-08-12T08:20:26.549344Z","shell.execute_reply":"2025-08-12T08:20:28.900073Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbcc53ba0dcc4f49b8f95313f0b91f0d"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-11-bc25cf7ecf92>:67: DeprecationWarning: Call to deprecated function create.\n  response = together.Complete.create(\n/usr/local/lib/python3.10/dist-packages/together/legacy/complete.py:23: UserWarning: The use of together.api_key is deprecated and will be removed in the next major release. Please set the TOGETHER_API_KEY environment variable instead.\n  warnings.warn(API_KEY_WARNING)\n","output_type":"stream"},{"name":"stdout","text":"⚖️ Legal Text Found:\n1. The Indian Penal Code (IPC) Section 80: This section outlines that an act is not considered an offence when it is done by accident or misfortune, without any criminal intention or knowledge, in the doing of a lawful act in a lawful manner by lawful means and with proper care and caution.\n\n    2. Circumstances of the Accident: The key factors that would determine whether your action counts as accidental or not are whether you acted with criminal intention or knowledge, if the act was lawful, done in a lawful manner, by lawful means, and with proper care and caution.\n\n    3. Establishing Absence of Criminal Intention: Make sure you can demonstrate that you did not intentionally cause the accident.\n\n    4. Lawful Act: The action you took should be one that is authorized by law.\n\n    5. Proper Care and Caution: Before undertaking the action, ensure you exercised the caution that a reasonably prudent person would exercise in similar circumstances to prevent an accident from occurring. If proven, it may offer some legal protection in case of an accident.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## location awareness","metadata":{}},{"cell_type":"code","source":"import requests\nimport json\nimport re\n\napi_key = \"AlzaSymw9AJ0q15jTp12FlRKCqCHrNHH54hWoxJ\"\nquery = \"law firms in dharwad\"\n\n\nurl = f\"https://maps.gomaps.pro/maps/api/place/textsearch/json?query={query}&key={api_key}\"\nresponse = requests.get(url).json()\n\n\nplaces = []\ndef get_photo_href(photo_info):\n    if photo_info:\n        attributions = photo_info.get(\"html_attributions\", [])\n        if attributions:\n            links = [re.search(r'href=\"([^\"]+)\"', attr).group(1) for attr in attributions if 'href=\"' in attr]\n            return links[0] if links else \"No attribution link\"\n    return \"No attribution link\"\n\n\nfor place in response.get(\"results\", []):\n    name = place.get(\"name\", \"Unknown\")\n    address = place.get(\"formatted_address\", \"No address found\")\n    rating = place.get(\"rating\", \"No rating\")\n\n    photo_info = place.get(\"photos\", [{}])[0]\n    reference = get_photo_href(photo_info)\n    \n    places.append(f\"{name}, Address: {address}, Rating: {rating}, reference:{reference}\")\n\n\nplaces_text = \"\\n\\n\".join(places)\n\n\nprompt = f\"\"\"\nBased on the given legal places, provide the reference, name, address ,rating  of the most relevant ones to the user:\n\n{places_text}\n\nOnly return the 3 most relevant results in a user-friendly way.\n\"\"\"\nresponse = together.Complete.create(\n        model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n        prompt=prompt,\n        max_tokens=500,\n        temperature=0\n    )\n    \n\nif \"choices\" in response and response[\"choices\"]:\n        \n        print(response[\"choices\"][0][\"text\"].strip())\nelse:\n        print( \"Error: No response received from Together AI.\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:20:28.901791Z","iopub.execute_input":"2025-08-12T08:20:28.902083Z","iopub.status.idle":"2025-08-12T08:20:30.481587Z","shell.execute_reply.started":"2025-08-12T08:20:28.902047Z","shell.execute_reply":"2025-08-12T08:20:30.480722Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-12-af620b77f971>:44: DeprecationWarning: Call to deprecated function create.\n  response = together.Complete.create(\n","output_type":"stream"},{"name":"stdout","text":"1. Name: The Law Offices of Omar Zambrano\n   Address: 614 W 5th St Ste 200, Los Angeles, CA 90071\n   Rating: 4.9 (out of 5)\n\n\n2. Name: The Law Offices of Joseph H. Low IV\n   Address: 11601 Wilshire Blvd #1600, Los Angeles, CA 90025\n   Rating: 4.8 (out of 5)\n\n\n3. Name: The Law Offices of Scott R. Antenstein\n   Address: 1900 Avenue of the Stars #2600, Los Angeles, CA 90067\n   Rating: 4.7 (out of 5)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## legal advice from lawyers","metadata":{}},{"cell_type":"code","source":"import json\nimport numpy as np\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# Load your dataset\nwith open(\"/kaggle/input/legal-advice/answers_data.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n\n# Storage containers\nprocessed_data = []\n\n\nfor entry in data:\n    full_text = entry[\"full_text\"]\n    answers = entry[\"answers\"]\n    question_url = entry[\"question_url\"]\n\n    # Join all answers into one string\n    joined_answers = \"\\n\".join(answers)\n\n    # Store only necessary fields\n    processed_data.append({\n        \"question_url\": question_url,\n        \"full_text\": full_text,\n        \"joined_answers\": joined_answers\n    })\n\n# Save cleaned and processed data\nwith open(\"processed_lawyer_data.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(processed_data, f, indent=2, ensure_ascii=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:20:30.482432Z","iopub.execute_input":"2025-08-12T08:20:30.482715Z","iopub.status.idle":"2025-08-12T08:20:30.553784Z","shell.execute_reply.started":"2025-08-12T08:20:30.482665Z","shell.execute_reply":"2025-08-12T08:20:30.552762Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:20:30.554564Z","iopub.execute_input":"2025-08-12T08:20:30.554821Z","iopub.status.idle":"2025-08-12T08:20:30.558970Z","shell.execute_reply.started":"2025-08-12T08:20:30.554800Z","shell.execute_reply":"2025-08-12T08:20:30.557809Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## summarize question and answer before embedding","metadata":{}},{"cell_type":"code","source":"import json\n\n# Load the JSON data\nwith open('/kaggle/working/processed_lawyer_data.json', 'r') as file:\n    data = json.load(file)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:20:30.559729Z","iopub.execute_input":"2025-08-12T08:20:30.559979Z","iopub.status.idle":"2025-08-12T08:20:30.584301Z","shell.execute_reply.started":"2025-08-12T08:20:30.559960Z","shell.execute_reply":"2025-08-12T08:20:30.583063Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from transformers import BartTokenizer, BartForConditionalGeneration\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\nmodel = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:20:30.585405Z","iopub.execute_input":"2025-08-12T08:20:30.585766Z","iopub.status.idle":"2025-08-12T08:20:41.696091Z","shell.execute_reply.started":"2025-08-12T08:20:30.585731Z","shell.execute_reply":"2025-08-12T08:20:41.695162Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc720ee9773b46b2af30a89e2544490f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48028b0cffcd46dcab9310576cc9f75e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6124b52e460e4fc0b02a89befe59a22f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2795bbf858a849ac9b6003d41078f9f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4aefa6475c98463b8c4c1f1da1cfaf68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c97de8faa1d54b0089f835dfb12c7cf1"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"model.save_pretrained(\"bart_summarizer\")\ntokenizer.save_pretrained(\"bart_summarizer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:20:41.697201Z","iopub.execute_input":"2025-08-12T08:20:41.697543Z","iopub.status.idle":"2025-08-12T08:20:44.903305Z","shell.execute_reply.started":"2025-08-12T08:20:41.697518Z","shell.execute_reply":"2025-08-12T08:20:44.902435Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"('bart_summarizer/tokenizer_config.json',\n 'bart_summarizer/special_tokens_map.json',\n 'bart_summarizer/vocab.json',\n 'bart_summarizer/merges.txt',\n 'bart_summarizer/added_tokens.json')"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import BartTokenizer, BartForConditionalGeneration\nfrom tqdm import tqdm\nimport torch\n\n# Load your dataset\ndf = pd.read_json(\"/kaggle/working/processed_lawyer_data.json\")  # Or use pd.read_csv(...) depending on your format\n\n# Load BART model and tokenizer\nmodel = BartForConditionalGeneration.from_pretrained(\"/kaggle/working/bart_summarizer\", forced_bos_token_id=0)\ntokenizer = BartTokenizer.from_pretrained(\"/kaggle/working/bart_summarizer\")\nmodel.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Function to summarize a single text\ndef summarize(text, max_input_length=1024, max_output_length=350):\n    prompt = (\n    \"You are a legal assistant. Summarize the following legal situation by extracting:\\n\"\n    \"- The legal dispute and its current status\\n\"\n    \"- Actions taken by the people involved\\n\"\n    \"- The legal question or help being asked\\n\\n\"\n    \"Text:\\n\" + text\n    )\n    inputs = tokenizer(prompt, max_length=1024, truncation=True, return_tensors=\"pt\").to(model.device)\n\n    summary_ids = model.generate(\n        inputs[\"input_ids\"],\n        num_beams=6,\n        max_length=max_output_length,\n        min_length=120,\n        no_repeat_ngram_size=3,\n        length_penalty=1.2,\n        early_stopping=True\n    )\n\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n# Prepare new columns for summaries\ndf[\"fulltext_summary\"] = \"\"\ndf[\"answers_summary\"] = \"\"\n\n# Summarize in batches\nfor idx in tqdm(range(len(df))):\n    try:\n        full = df.loc[idx, \"full_text\"]\n        ans = df.loc[idx, \"joined_answers\"]\n\n        df.at[idx, \"fulltext_summary\"] = summarize(full)\n        df.at[idx, \"answers_summary\"] = summarize(ans)\n    except Exception as e:\n        print(f\"Error at index {idx}: {e}\")\n        continue\n\n# Save the summarized dataset\ndf.to_json(\"summarized_legal_data.json\", orient=\"records\", indent=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:20:44.904353Z","iopub.execute_input":"2025-08-12T08:20:44.904723Z","iopub.status.idle":"2025-08-12T08:57:48.279766Z","shell.execute_reply.started":"2025-08-12T08:20:44.904669Z","shell.execute_reply":"2025-08-12T08:57:48.278798Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n  warnings.warn(\n  0%|          | 0/500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1527: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n100%|██████████| 500/500 [37:01<00:00,  4.44s/it]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"df = pd.read_json(\"/kaggle/working/summarized_legal_data.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:57:48.280792Z","iopub.execute_input":"2025-08-12T08:57:48.281106Z","iopub.status.idle":"2025-08-12T08:57:48.297757Z","shell.execute_reply.started":"2025-08-12T08:57:48.281068Z","shell.execute_reply":"2025-08-12T08:57:48.296950Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"final_legal_advice_data=df[[\"answers_summary\", \"fulltext_summary\", \"question_url\"]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:57:48.298809Z","iopub.execute_input":"2025-08-12T08:57:48.299044Z","iopub.status.idle":"2025-08-12T08:57:48.332674Z","shell.execute_reply.started":"2025-08-12T08:57:48.299025Z","shell.execute_reply":"2025-08-12T08:57:48.331766Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"final_legal_advice_data.to_json(\"final_summarized_data.json\", orient=\"records\", indent=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:57:48.333388Z","iopub.execute_input":"2025-08-12T08:57:48.333615Z","iopub.status.idle":"2025-08-12T08:57:48.357614Z","shell.execute_reply.started":"2025-08-12T08:57:48.333596Z","shell.execute_reply":"2025-08-12T08:57:48.356959Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## fine tuning inlegalbert","metadata":{}},{"cell_type":"code","source":"import json\n\nwith open(\"/kaggle/working/final_summarized_data.json\", \"r\") as f:\n    data = json.load(f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:57:48.358445Z","iopub.execute_input":"2025-08-12T08:57:48.358666Z","iopub.status.idle":"2025-08-12T08:57:48.376413Z","shell.execute_reply.started":"2025-08-12T08:57:48.358648Z","shell.execute_reply":"2025-08-12T08:57:48.375860Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, models\n\n# Load INLegalBERT model (from HuggingFace or local path)\nword_embedding_model = models.Transformer(\"law-ai/InLegalBERT\", max_seq_length=512)\n\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:57:48.377254Z","iopub.execute_input":"2025-08-12T08:57:48.377525Z","iopub.status.idle":"2025-08-12T08:57:49.317910Z","shell.execute_reply.started":"2025-08-12T08:57:48.377494Z","shell.execute_reply":"2025-08-12T08:57:49.316757Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from sentence_transformers import losses, InputExample, SentenceTransformer\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n\ntrain_examples = [\n    InputExample(texts=[item[\"fulltext_summary\"], item[\"answers_summary\"]], label=1) for item in        data\n]\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=4)\n\ntrain_loss = losses.CosineSimilarityLoss(model=model)\n\n# Step 6: Train the model\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    epochs=3,\n    warmup_steps=100,\n    show_progress_bar=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:57:49.319920Z","iopub.execute_input":"2025-08-12T08:57:49.320275Z","iopub.status.idle":"2025-08-12T08:59:17.186291Z","shell.execute_reply.started":"2025-08-12T08:57:49.320231Z","shell.execute_reply":"2025-08-12T08:59:17.185513Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [189/189 01:23, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"model.save(\"fine-tuned-inlegalbert\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:59:17.186946Z","iopub.execute_input":"2025-08-12T08:59:17.187207Z","iopub.status.idle":"2025-08-12T08:59:18.212163Z","shell.execute_reply.started":"2025-08-12T08:59:17.187185Z","shell.execute_reply":"2025-08-12T08:59:18.211442Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"## creating faiss index and cross encoder for similarity search","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"/kaggle/working/fine-tuned-inlegalbert\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:59:18.212960Z","iopub.execute_input":"2025-08-12T08:59:18.213187Z","iopub.status.idle":"2025-08-12T08:59:18.453994Z","shell.execute_reply.started":"2025-08-12T08:59:18.213168Z","shell.execute_reply":"2025-08-12T08:59:18.452979Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import faiss\nfull_texts = [item[\"fulltext_summary\"] for item in data]\nanswers = [item[\"answers_summary\"] for item in data]  # in case you want to use later\n\n# Embed all full_texts\nfull_text_embeddings = model.encode(full_texts, convert_to_numpy=True, show_progress_bar=True)\n\n# Create FAISS index (for cosine similarity use IndexFlatIP and normalize)\ndimension = full_text_embeddings.shape[1]\nindex = faiss.IndexFlatIP(dimension)\n\n# Normalize embeddings (important for cosine similarity)\nfaiss.normalize_L2(full_text_embeddings)\n\n# Add to FAISS\nindex.add(full_text_embeddings)\n\n# Save the index and texts (optional)\nfaiss.write_index(index, \"final_faiss_fulltext.index\")\nwith open(\"full_texts.json\", \"w\") as f:\n    json.dump(full_texts, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:59:18.457789Z","iopub.execute_input":"2025-08-12T08:59:18.458009Z","iopub.status.idle":"2025-08-12T08:59:23.274177Z","shell.execute_reply.started":"2025-08-12T08:59:18.457990Z","shell.execute_reply":"2025-08-12T08:59:23.273459Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8e5777ff3b64e08bbe44618d3971682"}},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"from sentence_transformers import CrossEncoder\n\n# Load a cross-encoder model trained for relevance ranking\nreranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:59:23.275143Z","iopub.execute_input":"2025-08-12T08:59:23.275372Z","iopub.status.idle":"2025-08-12T08:59:25.305780Z","shell.execute_reply.started":"2025-08-12T08:59:23.275352Z","shell.execute_reply":"2025-08-12T08:59:25.305133Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5666fae6cad9454195a92bfbdc483410"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7cb2de2bb624221bef881ad7d0cf6d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbb4f07dd22844e4866787fe49271f51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bc4d4a7276d4a54841d13ada130f8ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"720a4ccd4acf4813b291ff3c62d79e55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76825f2d64b7436ba52b02cc71121cb5"}},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"## legal advice","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport faiss\nindex=faiss.read_index(\"/kaggle/working/final_faiss_fulltext.index\")\ndef get_similar_advice(user_question, top_k=10, threshold=0.2):\n    # Step 1: Encode query\n    query_embedding = model.encode(user_question, convert_to_numpy=True)\n    query_embedding = np.array(query_embedding).reshape(1, -1).astype('float32')\n    faiss.normalize_L2(query_embedding)\n\n   \n    D, I = index.search(query_embedding, top_k)\n    similarity_scores = D[0]\n\n    candidates = []\n    for idx, i in enumerate(I[0]):\n        if i < len(data):\n            item = data[i].copy()\n            item[\"similarity\"] = float(similarity_scores[idx])\n            candidates.append(item)\n\n    # Step 3: Rerank\n    rerank_pairs = [(user_question, item[\"fulltext_summary\"]) for item in candidates]\n    scores = reranker.predict(rerank_pairs)\n\n    for i, item in enumerate(candidates):\n        item[\"rerank_score\"] = scores[i]\n\n    # Step 4: Sort by rerank score\n    sorted_candidates = sorted(candidates, key=lambda x: x[\"rerank_score\"], reverse=True)\n\n    \n    final_results = []\n    for item in sorted_candidates:\n        if item[\"similarity\"] >= threshold:\n            final_results.append({\n                \"answers\": item[\"answers_summary\"],\n                \"url\": item.get(\"question_url\", None),\n                \"similarity\": round(item[\"similarity\"], 3),\n                \"rerank_score\": round(item[\"rerank_score\"], 3)\n            })\n\n    if not final_results:\n        return [{\n            \"answers\": \"No relevant legal advice found for your query.\",\n            \"url\": None,\n            \"similarity\": 0,\n            \"rerank_score\": 0\n        }]\n\n    return final_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:59:25.306617Z","iopub.execute_input":"2025-08-12T08:59:25.306859Z","iopub.status.idle":"2025-08-12T08:59:25.315182Z","shell.execute_reply.started":"2025-08-12T08:59:25.306832Z","shell.execute_reply":"2025-08-12T08:59:25.314400Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"user_question = \"can i get forced to get divorce because of normal fights\"\nresponses = get_similar_advice(user_question)\n\nfor i, res in enumerate(responses, 1):\n    print(f\"\\n--- Response {i} ---\")\n    print(\"Answers:\\n\", res[\"answers\"])\n    print(\"URL:\", res[\"url\"])\n    print(\"Similarity:\", res[\"similarity\"])\n    print(\"Rerank Score:\", res[\"rerank_score\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:59:25.316239Z","iopub.execute_input":"2025-08-12T08:59:25.316534Z","iopub.status.idle":"2025-08-12T08:59:25.471894Z","shell.execute_reply.started":"2025-08-12T08:59:25.316507Z","shell.execute_reply":"2025-08-12T08:59:25.470786Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d72d57d5275b43d8a807bf0f8ba93114"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2158333c3117416882ca93ca4d5ec72d"}},"metadata":{}},{"name":"stdout","text":"\n--- Response 1 ---\nAnswers:\n Divorce is not a platform ticket, husband can go to Court and buy it across booking window. Law is in favor of woman. If he files divorce you can file number of cases, he will spend lot of money and time and lose peace of mind. Just ignore their conduct and tell them bluntly to proceed with their plans, you will take counter measure and make them pay for it. It is not that on 366 day husband file a divorce and he will get it. Only in cases of proved adultery, that too not just some incidents of adultery. For living in adultery a husband gets divorce that too after fighting for years in court.\nURL: https://www.kaanoon.com/486190/can-husband-force-me-to-give-him-divorce-because-of-normal-fights\nSimilarity: 0.972\nRerank Score: 7.029\n\n--- Response 2 ---\nAnswers:\n There's no obligation on you to take care or maintain your in laws or their extended family members. You can stop their entry into your home forcibly, if they become physical you may lodge a criminal complaint for trespassing your house. If your wife's behavior is intolerable and she has made your life miserable then you can file a divorce case on the grounds of cruelty. If she is filing any false case you can challenge them on merits properly instead of getting scared without taking any further steps. If you could spare two minutes of your time to write a review, it would be greatly appreciated and bring immense happiness to read it.\nURL: https://www.kaanoon.com/480991/my-wife-and-her-mother-along-with-siblings-planned-to-cause-trouble-me-and-my-old-age-parents\nSimilarity: 0.972\nRerank Score: -8.819\n\n--- Response 3 ---\nAnswers:\n If a wife wishes to avoid litigations she has initiated due to the reasons mentioned, it is crucial to adopt a balanced and practical approach. In the given circumstances, better to opt for Mutual Consent Divorce, which saves time and money. While the desire to avoid litigation is understandable, ensure that any withdrawal or settlement reflects fair and reasonable terms for both parties. Taking steps to secure financial, emotional, and social stability is key to moving forward. A qualified lawyer and counsellor can provide tailored guidance to navigate this challenging period. Thanks and Regards, Advocate Aman Verma, Legal Corridor.\nURL: https://www.kaanoon.com/482070/if-a-wife-wants-to-avoid-litigations-filed-by-her\nSimilarity: 0.972\nRerank Score: -11.012\n\n--- Response 4 ---\nAnswers:\n The first mistake you did was to get divorce in abroad than getting this in India. Had you done mutual divorce in India then there was no way you could have been beset with 498A or related cases. It is not clear whether you have filed quashing case or writ petition in high court. You first obtain anticipatory bail and visit India get enlarged on regular bail. After that you can either pursue the quash petition removal of LOCetc or file a discharge petition before the trial court for the reasons you rely upon. You may have to engage the services of a genuine lawyer with the help of some local people instead of approaching one by yourself.\nURL: https://www.kaanoon.com/479218/problem-with-lawyers\nSimilarity: 0.972\nRerank Score: -11.154\n\n--- Response 5 ---\nAnswers:\n According to the Indian Divorce Act, 1869, the custody of the minor children can be taken into consideration, during the divorce proceedings. Even though your husband does not consent for you moving abroad with your children, you may seek court's permission for the same. You need court orders to take your kids abroad if your husband refuses to give his consent.Taking a child without permission of a parent to abroad can be treated as crime if wife refuses to divulge any information about the child. You can apply for sole custody of your child under the provisions of guardians and wards act and also seek court order to take the children abroad wit you considering the children welfare as paramount.\nURL: https://www.kaanoon.com/476339/children-custody-and-moving-abroad-with-minor-kids\nSimilarity: 0.972\nRerank Score: -11.183\n\n--- Response 6 ---\nAnswers:\n It takes time in updation of orders in court websites. There is no reason to be panic. Judge might have fixed date but same has not been updated on court website. As such there is no delay on part of court. so you must file application for inspection. Also apply for certified copy of order of disposal.You need take circulation of the matter if it is not listed by Hc. If you want to speed the case then you need to apply the SC for the same same thing.Thank you for reaching out. Your concern is completely valid, especially since the judge explicitly prioritized the case.\nURL: https://www.kaanoon.com/484149/no-next-hearing-date-updated\nSimilarity: 0.973\nRerank Score: -11.334\n\n--- Response 7 ---\nAnswers:\n If majority of members are in favour of redevelopment court would not grant any stay. It is well settled law that few members cannot hold the redevelopment of building to ransom by refusing to vacate the premises. It's crucial to include a clause in the redevelopment agreement that specifies the developer's obligation to provide a bank guarantee before construction begins. The GR dated July 4, 2019, for redevelopment of cooperative housing societies issued by the Maharashtra government, provides for the points to be included in the development agreement (DA). Clause 18(2) provides that the developer shall give 20%of the total cost of the redevelopment project as bank guarantee (BG) to the society. Your society should insist on getting a BG as mandated.\nURL: https://www.kaanoon.com/486360/stay-redevelopment\nSimilarity: 0.973\nRerank Score: -11.358\n\n--- Response 8 ---\nAnswers:\n Cross-dressing is not considered an offence in India. Transgender people are legally recognized in India, and they are protected under the Transgender Persons (Protection of Rights) Act, 2019. Stay informed, avoid confrontations, and seek legal help if harassed. For detailed, personalized advice, consider a phone consultancy. You are free to contact me for further discussion. If you could spare two minutes of your time to write a review, it would be greatly appreciated and bring immense happiness to read it. Thank you. Shubham Goyal, a legal assistant at a law firm in Mumbai.\nURL: https://www.kaanoon.com/483534/crossdressing-crime-and-possible-action-in-indian-society\nSimilarity: 0.972\nRerank Score: -11.388\n\n--- Response 9 ---\nAnswers:\n You can enter into a lease agreement for ten years, but its registration is mandatory. You can mention there that in the event of non-payment of rent for even 2 months the agreement will be terminated and the landlord will have to evict the tenant with one month notice. It is advisable to engage a competent lawyer to draft the lease agreement suitably to include terms not prejudicial to your interests. You should engage a lawyer, if desired from this portal, for the drafting of a flawless commercial Leave and License Agreement to protect your rights. There can be no straight jacket formula that we can spell here.\nURL: https://www.kaanoon.com/478846/lease-term-for-small-to-medium-scale-industry\nSimilarity: 0.973\nRerank Score: -11.402\n\n--- Response 10 ---\nAnswers:\n Under Indian law, banks are required to comply with the provisions of the Information Technology Act, 2000, and the Reserve Bank of India (RBI) guidelines on cybersecurity. If a bank receives a cyber complaint or suspects any unauthorized or fraudulent activity related to a customer's account, it has the authority to temporarily block or freeze the account. The blocking of funds in your account could be due to the following reasons: Suspected Unauthorized Transaction: If the bank suspects that your account has been compromised or used for unauthorized transactions, it may block the funds as a security measure to prevent further misuse.\nURL: https://www.kaanoon.com/474750/i-have-been-victimised-in-cybercrime\nSimilarity: 0.972\nRerank Score: -11.419\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}